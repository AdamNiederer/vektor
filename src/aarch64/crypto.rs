// Autogenerated by `scrape.py`.
// See https://github.com/AdamNiederer/vektor-gen

#![allow(unused_imports)]
use crate::myarch::*;
use crate::simd::*;

/// AES single round encryption.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(aese))]
pub unsafe fn vaeseq_u8(data: uint8x16_t, key: uint8x16_t) -> uint8x16_t {
    crate::mem::transmute(crate::myarch::vaeseq_u8(crate::mem::transmute(data), crate::mem::transmute(key)))
}

/// AES single round decryption.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(aesd))]
pub unsafe fn vaesdq_u8(data: uint8x16_t, key: uint8x16_t) -> uint8x16_t {
    crate::mem::transmute(crate::myarch::vaesdq_u8(crate::mem::transmute(data), crate::mem::transmute(key)))
}

/// AES mix columns.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(aesmc))]
pub unsafe fn vaesmcq_u8(data: uint8x16_t) -> uint8x16_t {
    crate::mem::transmute(crate::myarch::vaesmcq_u8(crate::mem::transmute(data)))
}

/// AES inverse mix columns.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(aesimc))]
pub unsafe fn vaesimcq_u8(data: uint8x16_t) -> uint8x16_t {
    crate::mem::transmute(crate::myarch::vaesimcq_u8(crate::mem::transmute(data)))
}

/// SHA1 fixed rotate.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha1h))]
pub unsafe fn vsha1h_u32(hash_e: u32) -> u32 {
    crate::mem::transmute(crate::myarch::vsha1h_u32(crate::mem::transmute(hash_e)))
}

/// SHA1 hash update accelerator, choose.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha1c))]
pub unsafe fn vsha1cq_u32(hash_abcd: uint32x4_t, hash_e: u32, wk: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha1cq_u32(crate::mem::transmute(hash_abcd), crate::mem::transmute(hash_e), crate::mem::transmute(wk)))
}

/// SHA1 hash update accelerator, majority.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha1m))]
pub unsafe fn vsha1mq_u32(hash_abcd: uint32x4_t, hash_e: u32, wk: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha1mq_u32(crate::mem::transmute(hash_abcd), crate::mem::transmute(hash_e), crate::mem::transmute(wk)))
}

/// SHA1 hash update accelerator, parity.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha1p))]
pub unsafe fn vsha1pq_u32(hash_abcd: uint32x4_t, hash_e: u32, wk: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha1pq_u32(crate::mem::transmute(hash_abcd), crate::mem::transmute(hash_e), crate::mem::transmute(wk)))
}

/// SHA1 schedule update accelerator, first part.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha1su0))]
pub unsafe fn vsha1su0q_u32(w0_3: uint32x4_t, w4_7: uint32x4_t, w8_11: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha1su0q_u32(crate::mem::transmute(w0_3), crate::mem::transmute(w4_7), crate::mem::transmute(w8_11)))
}

/// SHA1 schedule update accelerator, second part.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha1su1))]
pub unsafe fn vsha1su1q_u32(tw0_3: uint32x4_t, w12_15: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha1su1q_u32(crate::mem::transmute(tw0_3), crate::mem::transmute(w12_15)))
}

/// SHA256 hash update accelerator.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha256h))]
pub unsafe fn vsha256hq_u32(hash_abcd: uint32x4_t, hash_efgh: uint32x4_t, wk: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha256hq_u32(crate::mem::transmute(hash_abcd), crate::mem::transmute(hash_efgh), crate::mem::transmute(wk)))
}

/// SHA256 hash update accelerator, upper part.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha256h2))]
pub unsafe fn vsha256h2q_u32(hash_efgh: uint32x4_t, hash_abcd: uint32x4_t, wk: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha256h2q_u32(crate::mem::transmute(hash_efgh), crate::mem::transmute(hash_abcd), crate::mem::transmute(wk)))
}

/// SHA256 schedule update accelerator, first part.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha256su0))]
pub unsafe fn vsha256su0q_u32(w0_3: uint32x4_t, w4_7: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha256su0q_u32(crate::mem::transmute(w0_3), crate::mem::transmute(w4_7)))
}

/// SHA256 schedule update accelerator, second part.
#[inline]
#[target_feature(enable = "crypto")]
#[cfg_attr(test, assert_instr(sha256su1))]
pub unsafe fn vsha256su1q_u32(tw0_3: uint32x4_t, w8_11: uint32x4_t, w12_15: uint32x4_t) -> uint32x4_t {
    crate::mem::transmute(crate::myarch::vsha256su1q_u32(crate::mem::transmute(tw0_3), crate::mem::transmute(w8_11), crate::mem::transmute(w12_15)))
}

