// Autogenerated by `scrape.py`.
// See https://github.com/AdamNiederer/vektor-gen

#![allow(unused_imports)]
use crate::myarch::*;
use crate::simd::*;

/// Unsigned multiply without affecting flags.
///
/// Unsigned multiplication of `a` with `b` returning a pair `(lo, hi)` with
/// the low half and the high half of the result.
#[inline]
#[cfg_attr(test, assert_instr(mulx))]
#[target_feature(enable = "bmi2")]
#[cfg(not(target_arch = "x86"))] // calls an intrinsic
pub unsafe fn _mulx_u64(a: u64, b: u64, hi: &mut u64) -> u64 {
    crate::mem::transmute(crate::myarch::_mulx_u64(crate::mem::transmute(a), crate::mem::transmute(b), crate::mem::transmute(hi)))
}

/// Zero higher bits of `a` >= `index`.
#[inline]
#[target_feature(enable = "bmi2")]
#[cfg_attr(test, assert_instr(bzhi))]
#[cfg(not(target_arch = "x86"))]
pub unsafe fn _bzhi_u64(a: u64, index: u32) -> u64 {
    crate::mem::transmute(crate::myarch::_bzhi_u64(crate::mem::transmute(a), crate::mem::transmute(index)))
}

/// Scatter contiguous low order bits of `a` to the result at the positions
/// specified by the `mask`.
#[inline]
#[target_feature(enable = "bmi2")]
#[cfg_attr(test, assert_instr(pdep))]
#[cfg(not(target_arch = "x86"))]
pub unsafe fn _pdep_u64(a: u64, mask: u64) -> u64 {
    crate::mem::transmute(crate::myarch::_pdep_u64(crate::mem::transmute(a), crate::mem::transmute(mask)))
}

/// Gathers the bits of `x` specified by the `mask` into the contiguous low
/// order bit positions of the result.
#[inline]
#[target_feature(enable = "bmi2")]
#[cfg_attr(test, assert_instr(pext))]
#[cfg(not(target_arch = "x86"))]
pub unsafe fn _pext_u64(a: u64, mask: u64) -> u64 {
    crate::mem::transmute(crate::myarch::_pext_u64(crate::mem::transmute(a), crate::mem::transmute(mask)))
}

